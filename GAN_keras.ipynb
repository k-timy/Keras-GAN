{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_keras.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO0JcMfmDycWRtCJ3834rCs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/k-timy/Keras-GAN/blob/master/GAN_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqjmIGXOS1IB",
        "colab_type": "text"
      },
      "source": [
        "## **Let's Setup the Environment first!**\n",
        "\n",
        "We need to install Tensorflow and Keras versions 2 (the newest versions are fine, however, they should not be before 2.1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sI2glRkBVkN",
        "colab_type": "code",
        "outputId": "4aeb8860-d2e8-4295-f545-181e5f29084d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Upgrading Colab's frameworks\n",
        "\n",
        "!pip install keras --upgrade\n",
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.1\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\r\u001b[K     |▉                               | 10kB 21.9MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 81kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 215kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 225kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 266kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 276kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 286kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 317kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 327kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 337kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 368kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 378kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.2.5\n",
            "    Uninstalling Keras-2.2.5:\n",
            "      Successfully uninstalled Keras-2.2.5\n",
            "Successfully installed keras-2.3.1\n",
            "Uninstalling tensorflow-1.15.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/freeze_graph\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-1.15.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow_core/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-1.15.0\n",
            "Collecting tensorflow==2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 41kB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (0.2.2)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/23/53ffe290341cd0855d595b0a2e7485932f473798af173bbe3a584b99bb06/tensorboard-2.1.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 31.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (3.1.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.4.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (0.9.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.17.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.0.8)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (0.33.6)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (3.10.0)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 60.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (0.1.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2.21.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.16.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (42.0.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.1.1)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/6d/7aae38a9022f982cf8167775c7fc299f203417b698c27080ce09060bba07/google_auth-1.11.0-py2.py3-none-any.whl (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1) (2.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (4.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.2.7)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.4.8)\n",
            "\u001b[31mERROR: tensorboard 2.1.0 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.11.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: google-auth, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: google-auth 1.4.2\n",
            "    Uninstalling google-auth-1.4.2:\n",
            "      Successfully uninstalled google-auth-1.4.2\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed google-auth-1.11.0 tensorboard-2.1.0 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_RDtDcJuE6m",
        "colab_type": "text"
      },
      "source": [
        "## **The Algorithm**\n",
        "\n",
        "Here is the algorithm from the paper:\n",
        "\n",
        "![alt text](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/Summary-of-the-Generative-Adversarial-Network-Training-Algorithm-1024x669.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlLM4Rxp3LLB",
        "colab_type": "text"
      },
      "source": [
        "## **Let's get into it!**\n",
        "\n",
        "The rest of this notebook is the implementation of the Generative Adversarial Network using multi-layered neural perceptrons (MLP). First the main libraries are imported:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCooGUHLlG43",
        "colab_type": "code",
        "outputId": "bda464db-3bf4-4e77-d5b3-19322acb5e5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# initial preprocessing image dimensions:\n",
        "img_rows, img_cols = 28, 28\n",
        "num_classes = 10\n",
        "\n",
        "# Just to make sure the tf version is 2.1.0 (or newer)\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0Ctzsa-exoJ",
        "colab_type": "text"
      },
      "source": [
        "## **Gimme the Data!**\n",
        "\n",
        "We load the set of images including 60000 training and 10000 testing handwritten images from the dataset of MNIST.\n",
        "\n",
        "Notice that the test set is also loaded here. However, it is not necessary for GAN to load the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-aKBJEOezzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if len(y_train.shape) < 2:\n",
        "  # convert class vectors to binary class matrices\n",
        "  # this \"if\" condition here, makes sure that the to_categorical function is \n",
        "  # only called once. And prevents the code from adding further dimensions\n",
        "  # to the y vectors(if the code is run again during the same runtime execution)\n",
        "  y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "  y_test = keras.utils.to_categorical(y_test, num_classes) \n",
        "\n",
        "# Since we are implementing an MLP, we convert the 2D images of size 28x28 into\n",
        "# 1D vectors of size 784 (=28x28)\n",
        "x_train = x_train.reshape(x_train.shape[0], img_rows * img_cols)\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows * img_cols)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI15tCtQ4S2x",
        "colab_type": "text"
      },
      "source": [
        "Taking a look at the shape of the loaded arrays of image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSpO4JXZxWW8",
        "colab_type": "code",
        "outputId": "f957e457-abc6-477b-bf89-78d071db3bfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "[x_train.shape ,y_train.shape]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(60000, 784), (60000, 10)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYdTADjo5DsH",
        "colab_type": "text"
      },
      "source": [
        "The imported data needs some preprocessing. Converting the image data to float data type and normalizing them to fall in range [0,1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joOHP_hHxeis",
        "colab_type": "code",
        "outputId": "219b53dd-fd47-47cf-fc96-36e83488b6aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 784)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qegxTeJk6-YB",
        "colab_type": "text"
      },
      "source": [
        "We define the Generator and the Discriminator classes here.\n",
        "Since both of these classes are **MLPs**, the pretty much cleaner and easier way to implement the structure of them is to use the sequential model as described in the official Keras's documentation [here](https://keras.io/getting-started/sequential-model-guide/):\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(32, input_shape=(784,)),\n",
        "    Activation('relu'),\n",
        "    Dense(10),\n",
        "    Activation('softmax'),\n",
        "])\n",
        "```\n",
        "However, I intentionally used  this approach of implementing a sequence of layers, in order to get my hands on this method of writing code as well. This method is helpful and necessary when writing custom architectures with probably several branches of computational graph.\n",
        "\n",
        "\n",
        "**Note:** For the architecture and hyper-parameters I used [this](https://github.com/lyeoni/pytorch-mnist-GAN/blob/master/pytorch-mnist-GAN.ipynb) and [this](https://github.com/eriklindernoren/Keras-GAN/blob/master/acgan/acgan.py) implementations as references for implementation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrZ-7XV3RySq",
        "colab_type": "text"
      },
      "source": [
        "## **The two Rivals! The Generator and The Discriminator**\n",
        "\n",
        "The following piece of code defines the MLPs of Generator and Discriminator.\n",
        "More explanations in the code!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV_9XP7JBzGo",
        "colab_type": "code",
        "outputId": "518b3ffb-cd53-41d5-b014-9e09693429d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Link: https://github.com/lyeoni/pytorch-mnist-GAN/blob/master/pytorch-mnist-GAN.ipynb\n",
        "\n",
        "class Generator(tf.keras.Model):\n",
        "  def __init__(self,latent_var_len, hidden_layer_len, output_size):\n",
        "    \"\"\"\n",
        "    The Generator class.\n",
        "    \n",
        "    To be used in GAN class as a property. That takes a vector of latent\n",
        "     variable and generates.\n",
        "      * `call()`: feeds the input of size `latent_var_len` to the generative MLP\n",
        "       and outputs an image in form of a vector of `output_size` dimensions. \n",
        "    \n",
        "    # Arguments\n",
        "        latent_var_len: The number of dimensions of the latent vector.\n",
        "        hidden_layer_len: The number of dimensions of the first hidden layer.\n",
        "          The other hidden layers will be twice as size of the previous hidden\n",
        "          layer in dimensions.\n",
        "        output_size: The number of dimensions of the vector that represents\n",
        "         a generated image. This needs to be converted to a 2D array, in order\n",
        "         to be visualized. For example for the MNIST dataset, this will be a\n",
        "         vector with length 784. That needs to be converted to a 2D array of \n",
        "         28 * 28.\n",
        "    \"\"\"\n",
        "    super(Generator, self).__init__()\n",
        "    \n",
        "    # Setting up the input layer\n",
        "    self.input_layer = keras.Input(shape=(latent_var_len,),\n",
        "                                   name='inp_latent_var')\n",
        "    \n",
        "    # Dense: layers are fully connected network (FCN)\n",
        "\n",
        "    # BatchNormalization: layers, perform normalization on the outputs of each\n",
        "    # FCN that results in faster convergence.\n",
        "    \n",
        "    # LeakyReLU : Provide unsaturated non-linearity so that the training speed\n",
        "    # increases.\n",
        "    self.dense1 = layers.Dense(hidden_layer_len,\n",
        "                               name='dense_1')(self.input_layer)\n",
        "    self.bo1 = layers.BatchNormalization()(self.dense1)\n",
        "    self.lr1 = layers.LeakyReLU(alpha=0.2)(self.bo1)\n",
        "\n",
        "    self.dense2 = layers.Dense(hidden_layer_len * 2,name='dense_2')(self.lr1)\n",
        "    self.bo2 = layers.BatchNormalization()(self.dense2)\n",
        "    self.lr2 = layers.LeakyReLU(alpha=0.2)(self.bo2)\n",
        "\n",
        "    self.dense3 = layers.Dense(hidden_layer_len * 4,name='dense_3')(self.lr2)\n",
        "    self.bo3 = layers.BatchNormalization()(self.dense3)\n",
        "    self.lr3 = layers.LeakyReLU(alpha=0.2)(self.bo3)\n",
        "\n",
        "    self.dense4 = layers.Dense(output_size, activation='tanh',\n",
        "                               name='dense_4')(self.lr3)\n",
        "    \n",
        "    # Wrapping all the computational graph in a single object so that it can\n",
        "    # be called in the __call__ function\n",
        "    self.gen = tf.keras.Model(inputs=self.input_layer, outputs=self.dense4)\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    return self.gen(inputs)\n",
        "  \n",
        "\n",
        "class Discriminator(tf.keras.Model):\n",
        "  def __init__(self,input_image_size, hidden_layer_len=1024):\n",
        "    \"\"\"\n",
        "    Discriminator class.\n",
        "    \n",
        "    To be used in GAN class. The purpose of this model is to identify the fake\n",
        "    images from the real ones.\n",
        "        * `call()`: Contains the logic for loss calculation using `y_true`, `y_pred`.\n",
        "    \n",
        "    # Arguments\n",
        "        input_image_size: The size of the image as a vector. i.e. width x height\n",
        "        hidden_layer_len: The size of the first hidden layer. The size of the \n",
        "        next hidden layers will be half of their previous ones.\n",
        "    \"\"\"\n",
        "\n",
        "    super(Discriminator, self).__init__()\n",
        "\n",
        "    # Dense: layers are Fully Connected Networks (FCN).\n",
        "    # LeakyReLU: as explained for the Generator class.\n",
        "    # Droput: Increases the regularization of the MLP by reducing overfitting.\n",
        "\n",
        "    # Setting up the input layer of the MLP\n",
        "    self.input_layer = keras.Input(shape=(input_image_size,),name='inp_image_var')\n",
        "    self.dense1 = layers.Dense(hidden_layer_len, name='dense_1')(self.input_layer)\n",
        "    self.lr1 = layers.LeakyReLU(alpha=0.2)(self.dense1)\n",
        "    self.do1 = layers.Dropout(rate=0.3)(self.lr1)\n",
        "\n",
        "    self.dense2 = layers.Dense(hidden_layer_len // 2,name='dense_2')(self.do1)\n",
        "    self.lr2 = layers.LeakyReLU(alpha=0.2)(self.dense2)\n",
        "    self.do2 = layers.Dropout(rate=0.5)(self.lr2)\n",
        "    \n",
        "    self.dense3 = layers.Dense(hidden_layer_len // 4, name='dense_3')(self.do2)\n",
        "    self.lr3 = layers.LeakyReLU(alpha=0.2)(self.dense3)\n",
        "    self.do3 = layers.Dropout(rate=0.3)(self.lr3)\n",
        "    \n",
        "    self.dense4 = layers.Dense(1, activation='sigmoid', name='dense_4')(self.do3)\n",
        "    \n",
        "    # Wrapping up the input and output as a single model.\n",
        "    self.disc = tf.keras.Model(inputs=self.input_layer, outputs=self.dense4)\n",
        "  \n",
        "  def __call__(self, inputs):\n",
        "    return self.disc(inputs)\n",
        "\n",
        "# Just to make sure that the code is run.\n",
        "print(Generator,Discriminator)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class '__main__.Generator'> <class '__main__.Discriminator'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmN0EWNqEI4W",
        "colab_type": "text"
      },
      "source": [
        "## **Let's Define Generative Adversarial Network!**\n",
        "\n",
        "The **Generative Adversarial Network (GAN)** is defined as a class in the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl6QSlZWEHxp",
        "colab_type": "code",
        "outputId": "00d349e2-82b0-4a61-9530-c16d3078add8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# A class for storing samples of generated images\n",
        "import imageio\n",
        "import os\n",
        "import time\n",
        "\n",
        "class MyGAN:\n",
        "  def __init__(self,image_size,img_classes, disc_hidden_layer_len,gen_hidden_layer_len,latent_var_size):\n",
        "    \"\"\" My implementation of the GANs.\n",
        "    \n",
        "    # Arguments\n",
        "        image_size: The size of the image as a vector. i.e. width x height\n",
        "        img_classes: Number of image classes. (Not used in this implementation)\n",
        "        disc_hidden_layer_len: The number of nodes in the first hidden layer of\n",
        "        the discriminator.\n",
        "        gen_hidden_layer_len: The number of nodes in the first hidden layer of \n",
        "        the generator.\n",
        "        latent_var_size: The size of the latent variable vector.\n",
        "    \"\"\"\n",
        "    super(MyGAN,self).__init__()\n",
        "\n",
        "    # Initializing Generator and Discriminator given the values.  \n",
        "    self.generator = Generator(latent_var_size,gen_hidden_layer_len,image_size)\n",
        "    self.discriminator = Discriminator(image_size,hidden_layer_len=disc_hidden_layer_len)\n",
        "\n",
        "    # Setting up some values as properties\n",
        "    self.latent_var_size = latent_var_size\n",
        "    self.image_classes = img_classes\n",
        "    self.image_size = image_size\n",
        "\n",
        "    # Setting up loss functions. Since there are only two classes of images in\n",
        "    # This implementation of GAN (fake=0, real=1), we consider using \n",
        "    # BinaryCrossEntropy\n",
        "    self.gen_loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    self.disc_loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "\n",
        "    # Setting up optimiziers for each of the MLPs\n",
        "    self.gen_opt = tf.keras.optimizers.Adam(learning_rate=2e-4,beta_1=0.5)\n",
        "    self.disc_opt = tf.keras.optimizers.Adam(learning_rate=2e-4,beta_1=0.5)\n",
        "\n",
        "  def __train_generator_one_batch(self,x):        \n",
        "    \"\"\"\n",
        "    Train the generator with one batch of input images (x). Though, only the\n",
        "    size of batch is used for this training and the generator does not have\n",
        "    direct access to the images of the dataset. It is only trained based on the\n",
        "    gradients passed from the discriminator.\n",
        "    \"\"\"\n",
        "\n",
        "    # Keeping track of the computations in a tape:\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "      # Drawing N samples of latent vectors with normal distribution.\n",
        "      # where N is the size of the batches.\n",
        "      z = tf.keras.backend.random_normal((x.shape[0],self.latent_var_size))\n",
        "\n",
        "      # Deceiving the discriminator by assiging the class of real images\n",
        "      # to fake images\n",
        "      y = tf.ones(x.shape[0],1)\n",
        "\n",
        "      # generating images\n",
        "      gen_out = self.generator(z)\n",
        "\n",
        "      # classifying the generated images\n",
        "      disc_out = self.discriminator(gen_out)\n",
        "\n",
        "      # calculating the loss of classification to be passed to the generator\n",
        "      gen_loss = self.gen_loss_fn(y, disc_out)\n",
        "    \n",
        "    # calculating gradients of the generator weights based on the loss\n",
        "    grads = tape.gradient(gen_loss, self.generator.trainable_weights)\n",
        "\n",
        "    # updating the weights of the generator using the gradients.\n",
        "    self.gen_opt.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "    # returning the loss of classification of generated samples\n",
        "    return gen_loss.numpy()\n",
        "\n",
        "  def __train_discriminator_one_batch(self,x):\n",
        "    \"\"\"\n",
        "      Train the discriminator with one batch of real image samples (x) \n",
        "      through the adversarial process.\n",
        "    \"\"\"\n",
        "\n",
        "    # Keeping track of the computations in a tape:\n",
        "    with tf.GradientTape() as tape:        \n",
        "      \n",
        "      # train discriminator on real data\n",
        "      x_real, y_real = x, tf.ones((x.shape[0],1))\n",
        "      disc_real_out = self.discriminator(x_real)\n",
        "      disc_real_loss = self.disc_loss_fn(y_real,disc_real_out)\n",
        "    \n",
        "      # train discriminator on fake data\n",
        "\n",
        "      # drawing samples of latent variables\n",
        "      z = tf.keras.backend.random_normal((x.shape[0],self.latent_var_size))\n",
        "      \n",
        "      # generating fake images from the latent variables given\n",
        "      x_fake = self.generator(z)\n",
        "      y_fake = tf.zeros((x.shape[0],1))\n",
        "\n",
        "      # calculating loss of classification of fake images\n",
        "      disc_fake_out = self.discriminator(x_fake)\n",
        "      disc_fake_loss = self.disc_loss_fn(y_fake,disc_fake_out)\n",
        "\n",
        "      # sum both losses of fake and real classifications\n",
        "      disc_loss_total = disc_fake_loss + disc_real_loss\n",
        "    \n",
        "    # calculating the gradients of the discriminator from the total loss \n",
        "    # of classifications\n",
        "    grads = tape.gradient(disc_loss_total, self.discriminator.trainable_weights)\n",
        "\n",
        "    # updating weights of the discriminator MLP\n",
        "    self.disc_opt.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
        "\n",
        "    # returning the discriminator loss\n",
        "    return disc_loss_total.numpy()\n",
        "\n",
        "  def train_one_batch(self,x,disc_runs=1):\n",
        "    \"\"\"\n",
        "      Train the GAN using the algorithm described in the paper of GAN,\n",
        "      given a batch of input images (x)\n",
        "    \"\"\"\n",
        "\n",
        "    # train discriminator for `disc_runs` epochs. The default value is 1 and it\n",
        "    # works well. However, I wrote this code to follow the algorithm\n",
        "    # described in GAN paper.\n",
        "\n",
        "    d_losses = []\n",
        "    for i in range(disc_runs):\n",
        "      d_losses.append(self.__train_discriminator_one_batch(x))\n",
        "    \n",
        "    # train the generator\n",
        "    \n",
        "    g_loss = self.__train_generator_one_batch(x)\n",
        "    d_loss = np.asarray(d_losses).mean()\n",
        "\n",
        "    return [d_loss, g_loss]\n",
        "\n",
        "  def sample_images(self, epoch):\n",
        "    \"\"\"\n",
        "      Sample 200 images from the GAN and store them as a single image file of\n",
        "       20x10 tiles of small images. The `epoch` argument is only passed so\n",
        "      that the saved images can be distinguished from each other.\n",
        "\n",
        "      The function returns the name of stored image as an string. \n",
        "    \"\"\"\n",
        "    r, c = 20, 10\n",
        "    \n",
        "    # Draw r x c latent samples\n",
        "    z = tf.keras.backend.random_normal((r * c,self.latent_var_size))\n",
        "\n",
        "    # Generate images\n",
        "    x_fake = self.generator(z)\n",
        "    \n",
        "    # Rescale images into the range of [0,1]\n",
        "    gen_imgs = 0.5 * x_fake.numpy() + 0.5\n",
        "\n",
        "    # Reshape the images into an array containing 200 2D images of size 28 x 28\n",
        "    gen_imgs = gen_imgs.reshape(x_fake.shape[0],28,28)\n",
        "\n",
        "    # placing images on a big image of size (r x 28) x (c x 28)\n",
        "    canvas = np.zeros((r * 28,c * 28))\n",
        "    \n",
        "    # index of image on gen_imgs array\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            # storing each image on its respective place on canvas            \n",
        "            canvas[i * 28:(i+1) * 28,j * 28:(j+1) * 28] = gen_imgs[cnt,:,:]\n",
        "            cnt += 1\n",
        "    \n",
        "    fname = 'samples_from_my_gan_epoch_{}.png'.format(epoch)\n",
        "    \n",
        "    # saving image file\n",
        "    imageio.imwrite(fname,canvas)\n",
        "    return fname\n",
        "    \n",
        "# Just to make sure the code is run. Some times you think you have clicked on\n",
        "# the run button, but in fact you have not :D\n",
        "print(MyGAN)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class '__main__.MyGAN'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2hc9MksTcu8",
        "colab_type": "text"
      },
      "source": [
        "## **MyGAN Class in Action!**\n",
        "\n",
        "Let's see how this GAN class performs. I ran this code and in 50 epochs it works fine. At first, a dataset object is created and then during the epochs it\n",
        "is trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Kqwk-BNTbht",
        "colab_type": "code",
        "outputId": "d60309d9-589f-4d5b-f65b-a6b0fad7a69e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# create an instance of teh class\n",
        "mygan = MyGAN(28*28,10,1024,256,100)\n",
        "\n",
        "# Prepare the training dataset\n",
        "batch_size = 64\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "\n",
        "# For debugging purposes\n",
        "break_loops = False\n",
        "\n",
        "# Iterate over epochs.\n",
        "epochs = 50\n",
        "file_names = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  \n",
        "  if break_loops:\n",
        "    break\n",
        "\n",
        "  print('Start of epoch {}'.format(epoch))\n",
        "\n",
        "  d_losses = []\n",
        "  g_losses = []\n",
        "\n",
        "  # Iterate over the batches of the dataset.\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "    dl,gl = mygan.train_one_batch(x_batch_train)\n",
        "    d_losses.append(dl)\n",
        "    g_losses.append(gl)    \n",
        "\n",
        "  # aggregate losses\n",
        "  d_losses = np.asarray(d_losses)\n",
        "  g_losses = np.asarray(g_losses)\n",
        "\n",
        "  # Sample some images and store them\n",
        "  if epoch % 5 == 0:\n",
        "    file_names.append(mygan.sample_images(epoch))\n",
        "    break\n",
        "  print('epoch {} : disc: {:.4f} gen: {:.4f}'.format(epoch,d_losses.mean(),g_losses.mean()))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKQd0q_2PmXb",
        "colab_type": "text"
      },
      "source": [
        "## **Where the images at?**\n",
        "\n",
        "Now that the training process is completed. Lets take a look at how the generated images actually look like! The following code, downloads the samples of multliplications of 5:\n",
        "\n",
        "**Note:** In order to run the files.download() function of google colab properly, you might need to allow the **colab.research.google.com** website to use the 3rd party cookies on your google chrome. As explained [here](https://stackoverflow.com/questions/53581023/google-colab-file-download-failed-to-fetch-error). Otherwise, you might get some errors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm74jcKG_4ac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "for f in file_names:  \n",
        "  files.download(f)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjVNtuf5RMMj",
        "colab_type": "text"
      },
      "source": [
        "I hope this piece of code helps you in getting started with the Keras and Tensorflow library. Please let me know of your feedbacks. Thanks."
      ]
    }
  ]
}